{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model prelim notebook\n",
    "\n",
    "Use this notebook to walk through a model to mak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess raw data and save to local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download from:\n",
    "https://archive.org/details/stackexchange\n",
    "\n",
    "- comes in *.tar.7z\n",
    "- you can download 7zip from their downloads page for linux\n",
    "- you can unzip using bzip2\n",
    "- you can run 7z binary, for example: `p7zip_16.02/bin/7z e philosophy.stackexchange.com.7z`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from random import shuffle\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')\n",
    "\n",
    "from backend.parse_stack_exchange import parse_xml_doc, preprocess_a_text, get_texts, write_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmls = {'Badges.xml': None,\n",
    "        'Comments.xml': 'Text',\n",
    "        'PostHistory.xml': None,\n",
    "        'PostLinks.xml': None,\n",
    "        'Posts.xml': 'Body',\n",
    "        'Tags.xml': None,\n",
    "        'Users.xml': None,\n",
    "        'Votes.xml': None}\n",
    "stack_dir = os.path.join('data', 'stack_exchange')\n",
    "stack_list= ['monero.stackexchange.com',\n",
    "             'movies.stackexchange.com',\n",
    "             'philosophy.stackexchange.com',\n",
    "             'politics.stackexchange.com']\n",
    "datatype = 'Comments.xml'\n",
    "childs = [None, 'Text', None, None, 'Body', None, None, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(stack_dir, stack_list[2], datatype)\n",
    "docs = parse_xml_doc(path, xmls[datatype])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "preprocessed_docs = [preprocess_a_text(doc) for doc in docs]\n",
    "\n",
    "questions = get_texts(preprocessed_docs, get_questions=True)\n",
    "non_questions = get_texts(preprocessed_docs, get_questions=False)\n",
    "\n",
    "shuffle(questions)\n",
    "shuffle(non_questions)\n",
    "\n",
    "# equalize list lengths\n",
    "non_questions = non_questions[:len(questions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "write_data(questions, non_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model - Attention 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you take a vector and normalize it so that it sums to 1 (prob distirbution style) then you can take a measure: Entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "\n",
    "from models.attention_1.model import convert_prediction_to_sentence\n",
    "from backend.preprocess import create_features, get_raw_vocab, read_files, process_glove\n",
    "from backend.data_gather import gather_stack_exchange_from_file, make_model_dir\n",
    "\n",
    "from models.attention_1.model import model_fn, input_fn, pred_input_fn, serving_input_receiver_fn, convert_prediction_to_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = [\"./data/QUESTIONS.txt\", \"./data/NON_QUESTIONS.txt\"]\n",
    "\n",
    "BATCH_SIZE = 5\n",
    "GLOVE = './data/glove.6B.50d.txt'\n",
    "MAX_SEQ_LEN = 25\n",
    "questions, non_questions = gather_stack_exchange_from_file(*file_names)\n",
    "vocab, vectors, word2index = process_glove(glove_file=GLOVE)\n",
    "index2word = {idx: word for word, idx in word2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERWRITE = False\n",
    "if not os.path.exists('features.pkl') or not os.path.exists('labels.pkl') or OVERWRITE:\n",
    "    features, labels = create_features(questions,\n",
    "                                       non_questions,\n",
    "                                       word2index,\n",
    "                                       max_seq_length=MAX_SEQ_LEN,\n",
    "                                       size=5)\n",
    "    with open('features.pkl', 'wb+') as fe, open('labels.pkl', 'wb+') as la:\n",
    "        pickle.dump(features, fe)\n",
    "        pickle.dump(labels, la)\n",
    "else:\n",
    "    with open('features.pkl', 'rb') as fe, open('labels.pkl', 'rb') as la:\n",
    "        features = pickle.load(fe)\n",
    "        labels = pickle.load(la)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make toy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERWRITEsm = False\n",
    "if not os.path.exists('small_features.pkl') or not os.path.exists('small_labels.pkl') or OVERWRITEsm:\n",
    "\n",
    "    smallword2index = dict()\n",
    "    smallvocab = list()\n",
    "    idx_set = list(set([x for a in features[0] for x in a] + [0, 1, 2, word2index['<UNK>']] + [z for b in labels[0] for z in b]))\n",
    "    for idx in idx_set:\n",
    "\n",
    "        if idx in word2index.values():\n",
    "            smallword2index[index2word[idx]] = idx\n",
    "            smallvocab.append(index2word[idx])\n",
    "\n",
    "    s2 = {idx: word for word, idx in smallword2index.items()}\n",
    "\n",
    "    newidx2word = {idx: word for idx, word in enumerate(list(s2.values()))}\n",
    "    newword2idx = {word: idx for idx, word in newidx2word.items()}\n",
    "\n",
    "    newlist = list()\n",
    "    for f in features[0]:\n",
    "        words = [index2word[x] for x in f]\n",
    "        newlist.append([newword2idx[x] for x in words])\n",
    "\n",
    "    newlist2 = list()\n",
    "    for g in labels[0]:\n",
    "        words = [index2word[x] for x in g]\n",
    "        newlist2.append([newword2idx[ii] for ii in words])\n",
    "\n",
    "    small_features = (newlist, features[1], features[2], features[3])\n",
    "    small_labels = (newlist2, labels[1], labels[2])\n",
    "\n",
    "    with open('small_features.pkl', 'wb+') as fe, open('small_labels.pkl', 'wb+') as la:\n",
    "        pickle.dump(features, fe)\n",
    "        pickle.dump(labels, la)\n",
    "else:\n",
    "    with open('notebooks/small_features.pkl', 'rb') as fe, open('notebooks/small_labels.pkl', 'rb') as la:\n",
    "        small_features = pickle.load(fe)\n",
    "        small_labels = pickle.load(la)\n",
    "\n",
    "        smallword2index = dict()\n",
    "        smallvocab = list()\n",
    "        idx_set = list(set([x for a in small_features[0] for x in a] + [0, 1, 2, word2index['<UNK>']] + [z for b in small_labels[0] for z in b]))\n",
    "        for idx in idx_set:\n",
    "\n",
    "            if idx in word2index.values():\n",
    "                smallword2index[index2word[idx]] = idx\n",
    "                smallvocab.append(index2word[idx])\n",
    "        s2 = {idx: word for word, idx in smallword2index.items()}\n",
    "\n",
    "        newidx2word = {idx: word for idx, word in enumerate(list(s2.values()))}\n",
    "        newword2idx = {word: idx for idx, word in newidx2word.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "graph_params = {\n",
    "    'num_classes': 4,\n",
    "    'vocab_size': len(smallvocab),#len(vocab),\n",
    "    'embed_dim': 50,\n",
    "    'num_units': 50,\n",
    "    'input_max_length': MAX_SEQ_LEN,\n",
    "    'output_max_length': MAX_SEQ_LEN,\n",
    "    'forget_bias': 0.6,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'glove_file': GLOVE,\n",
    "    'embedding_vectors': vectors,\n",
    "    'word2index': newword2idx,#word2index,\n",
    "    'index2word': newidx2word,#index2word,\n",
    "    'vocab': smallvocab,#vocab,\n",
    "    'decode_mode': True\n",
    "\n",
    "}\n",
    "data_params = {\n",
    "    'shuffle': 1,\n",
    "    'glove_file': GLOVE,\n",
    "    'repeat': -1,\n",
    "    'batch_size': BATCH_SIZE\n",
    "}\n",
    "model_dir = make_model_dir(name='attention1-1_trainable_embeds_log_finalstate',\n",
    "                           overwrite=True)\n",
    "config_dict = {'model_dir': model_dir,\n",
    "               'tf_random_seed': 42,\n",
    "               'save_summary_steps': 1000,\n",
    "               'save_checkpoints_steps': 200,\n",
    "               'keep_checkpoint_max': 5,\n",
    "               'keep_checkpoint_every_n_hours': 10000,\n",
    "               'log_step_count_steps': 2000,\n",
    "               'train_distribute': None,\n",
    "         }\n",
    "config = tf.estimator.RunConfig(**config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "classifier = tf.estimator.Estimator(model_fn=model_fn, params=graph_params, config=config)\n",
    "classifier.train(steps=100000, input_fn=lambda: input_fn(small_features, small_labels, data_params));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the model for serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serving_input_receiver_fn1():\n",
    "    feature_placeholders  ={\n",
    "      'encoder_inputs' : tf.placeholder(tf.int32, [None, MAX_SEQ_LEN]),\n",
    "      'encoder_input_lengths' : tf.placeholder(tf.int32, [None])}\n",
    "    features = {\n",
    "            key: tensor\n",
    "            for key, tensor in feature_placeholders.items()\n",
    "        }\n",
    " \n",
    "    return tf.estimator.export.ServingInputReceiver(features, \n",
    "                                                    feature_placeholders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "export_dir = classifier.export_savedmodel('TEST_EXPORT',\n",
    "                                          serving_input_receiver_fn=serving_input_receiver_fn1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = tf.estimator.Estimator(model_fn=model_fn, params=graph_params, model_dir=model_dir, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_sent(ind, index2word):\n",
    "    return \" \".join([index2word[x] for x in ind.tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = classifier.predict(input_fn=lambda: pred_input_fn(small_features),\n",
    "                       checkpoint_path=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ha = list()\n",
    "for i in x:\n",
    "    ha.append(convert_prediction_to_sentence(i, newidx2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for u in small_features[0]:\n",
    "    print(\" \".join([newidx2word[x] for x in u])); print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for u in small_labels[0]:\n",
    "    print(\" \".join([newidx2word[x] for x in u])); print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.ops.init_ops.TruncatedNormal at 0x1a940a88080>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.initializers.truncated_normal(0.0, .001, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.contrib.seq2seq.dynamic_decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
