{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data generator notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import signal\n",
    "import warnings\n",
    "from collections import OrderedDict\n",
    "from functools import partial\n",
    "from multiprocessing import Process, Queue\n",
    "\n",
    "from random import shuffle\n",
    "from time import sleep\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from backend.special_tokens import SPECIAL_TOKENS\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "from argparse import ArgumentParser\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from tensorflow import logging as logging\n",
    "\n",
    "from backend.container import ArgContainer\n",
    "from backend.config import (load_estimator_config, load_graph_params,\n",
    "                            load_input_fn_params)\n",
    "from backend.input_functions import (eval_input_fn,  # noqa\n",
    "                                     train_generator_input_fn,\n",
    "                                     train_generator_input_fn_v2,\n",
    "                                     train_static_input_fn)\n",
    "from backend.utils import check_response, convert_int2word, make_model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GeneratorInputV3(object):\n",
    "    \"\"\"\n",
    "    NEW MODEL GENERATOR\n",
    "\n",
    "    This input function will ALSO return a start position and an end position to identify where the question starts and ends.\n",
    "\n",
    "    ** V2 results in approximately 70% faster data loading than v1 **\n",
    "    A sample generator with a side process to fill a que of samples. tensorflow can grab items from the queue generator at will.\n",
    "\n",
    "    Usage:\n",
    "\n",
    "    gen_obj = GneratorInputV2(**kwargs)  # This should take max a couple minutes to prefill the queue\n",
    "    generator = gen_obj.from_queue_generator()\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 Q_size,\n",
    "                 num_proc,\n",
    "                 questions,\n",
    "                 non_questions,\n",
    "                 word2index,\n",
    "                 max_seq_length,\n",
    "                 max_num_questions,\n",
    "                 max_num_elements,\n",
    "                 randomize_num_questions=False):\n",
    "\n",
    "        print('Initializing generator')\n",
    "        self.global_sleep_clock = 2\n",
    "\n",
    "#         self.queues_length = 0\n",
    "        self.Q_size = Q_size\n",
    "        self.data_que = Queue(Q_size)\n",
    "\n",
    "        self.processes = [Process(target=self.queue_builder,\n",
    "                                  args=(questions,\n",
    "                                        non_questions,\n",
    "                                        word2index,\n",
    "                                        max_seq_length,\n",
    "                                        max_num_questions,\n",
    "                                        max_num_elements,\n",
    "                                        randomize_num_questions)) for _ in range(num_proc)]\n",
    "        self.initialized = False\n",
    "        import pdb;pdb.set_trace()\n",
    "        # cleanup from previous iteration\n",
    "        prev_pids = self.read_prev_pids()\n",
    "        self.kill_prev_pids(prev_pids)\n",
    "\n",
    "        # start the engines\n",
    "        self.start_processes()\n",
    "        self.save_pids()\n",
    "\n",
    "        print('Process started')\n",
    "        while not self.data_que.full():\n",
    "            size = self.queue_size()\n",
    "            print(self.data_que.get())\n",
    "            print('Queues initializing...{} of {}'.format(str(size), self.Q_size))\n",
    "            sleep(self.global_sleep_clock)\n",
    "        self.initialized = True\n",
    "\n",
    "    def save_pids(self):\n",
    "        pids = [p.pid for p in self.processes]\n",
    "        with open('temp_pid', 'w+') as pout:\n",
    "            for pid in pids:\n",
    "                pout.write(str(pid) + '\\n')\n",
    "\n",
    "    def read_prev_pids(self):\n",
    "        try:\n",
    "            with open('temp_pid', 'r') as pin:\n",
    "                pid_list = [int(x.strip()) for x in pin.readlines()]\n",
    "            os.remove('temp_pid')\n",
    "        except IOError:\n",
    "            pid_list = None\n",
    "        return pid_list\n",
    "\n",
    "    def kill_prev_pids(self, pid_list):\n",
    "        if pid_list:\n",
    "            try:\n",
    "                for pid in pid_list:\n",
    "                    os.kill(pid, signal.SIGTERM)\n",
    "            except Exception as e:\n",
    "                pass\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    def terminate_processes(self):\n",
    "        for proc in self.processes:\n",
    "            proc.terminate()\n",
    "\n",
    "    def start_processes(self):\n",
    "        for proc in self.processes:\n",
    "            proc.start()\n",
    "\n",
    "    def queue_size(self):\n",
    "        return self.data_que.qsize()\n",
    "\n",
    "    def kill_builder(self):\n",
    "        self.processes.terminate()\n",
    "\n",
    "    def make_breaks(self, input_list):\n",
    "        starts = list()\n",
    "        stops = list()\n",
    "\n",
    "        start = 0\n",
    "        for length, sentence in input_list:\n",
    "            stop = start + length -1\n",
    "\n",
    "            if sentence.endswith('<QQQ>'):\n",
    "                starts.append(start)\n",
    "                stops.append(stop)\n",
    "\n",
    "            start += length\n",
    "\n",
    "        return starts[0], stops[0]\n",
    "\n",
    "    def _generate_sequence_(self, questions, non_questions, max_seq_length, num_questions, max_num_elements):\n",
    "        \"\"\"\n",
    "        - Aim to generate seq shorter than max_seq_length by filtering as data is processed\n",
    "        \"\"\"\n",
    "        assert max_num_elements >= num_questions, 'must have more elements than questions'\n",
    "\n",
    "        num_non_questions = max_num_elements - num_questions\n",
    "\n",
    "        # keep track of questions for label making\n",
    "        pre_choices = np.random.choice(questions, num_questions, replace=False).tolist()\n",
    "        question_choices = [''.join([x, '<QQQ>']) for x in pre_choices]\n",
    "        length_of_questions = sum([len(x.split()) for x in question_choices])\n",
    "\n",
    "        filtered_non_questions = list(filter(lambda x: len(x.split()) <= int((max_seq_length - length_of_questions) / num_non_questions), non_questions))\n",
    "        shuffle(filtered_non_questions)\n",
    "\n",
    "        non_question_choices = np.random.choice(filtered_non_questions, num_non_questions, replace=False).tolist()\n",
    "\n",
    "        # New code\n",
    "        input_list = [(len(x.split()), x) for x in question_choices] + [(len(x.split()), x) for x in non_question_choices]\n",
    "\n",
    "        shuffle(input_list)\n",
    "\n",
    "        # New code\n",
    "        starts, stops = self.make_breaks(input_list)\n",
    "        input_list = [x[1] for x in input_list]\n",
    "\n",
    "        input_sequence = ' '.join(input_list).replace('<QQQ>', '')\n",
    "        target_sequence = ' '.join(\n",
    "            [x.replace('<QQQ>', '') for x in input_list if '<QQQ>' in x])\n",
    "        return input_sequence, target_sequence, num_questions, starts, stops\n",
    "\n",
    "\n",
    "    def queue_builder(self,\n",
    "                      questions,\n",
    "                      non_questions,\n",
    "                      word2index,\n",
    "                      max_seq_length,\n",
    "                      max_num_questions,\n",
    "                      max_num_elements,\n",
    "                      randomize_num_questions):\n",
    "        \" This function gets its own process which will run on the side \"\n",
    "\n",
    "        # to ensure no two questions togther reach the max_seq_length\n",
    "        filtered_questions = list(filter(lambda x: len(x.split()) <= max_seq_length // max_num_elements, questions))\n",
    "        shuffle(filtered_questions)\n",
    "\n",
    "        assert max_num_questions <= max_num_elements - 1, ' need to have at least 1 fewer questions than num element '  # to ensure that there will always be non-questions\n",
    "        while True:\n",
    "            if not self.data_que.full():\n",
    "                # if self.initialized:\n",
    "                    # print(\"Queue not full. Repleneshing...\")\n",
    "                if randomize_num_questions:  # if max elements is 3, then we can add 1 or 2 questions.\n",
    "                        max_num_questions = np.random.randint(1, max_num_elements - 1)\n",
    "\n",
    "                input_sequence, target_sequence, num_questions, starts_list, stops_list = self._generate_sequence_(filtered_questions,\n",
    "                                                                                                                    non_questions,\n",
    "                                                                                                                    max_seq_length=max_seq_length,\n",
    "                                                                                                                    num_questions=max_num_questions,\n",
    "                                                                                                                    max_num_elements=max_num_elements)\n",
    "\n",
    "                # if _is_acceptable_(input_sequence, max_seq_length):\n",
    "                encoder_inputs, encoder_input_lengths = _prepare_encoder_input_(input_sequence, max_seq_length, word2index)\n",
    "                decoder_inputs, decoder_input_lengths = _prepare_decoder_input_(target_sequence, max_seq_length, word2index)\n",
    "                target_sequences, target_seq_lengths = _prepare_target_sequences_(target_sequence, max_seq_length, word2index)\n",
    "\n",
    "                array = partial(np.array, dtype=np.int32)\n",
    "                features = array(encoder_inputs), array(encoder_input_lengths), array(decoder_inputs), array(decoder_input_lengths)\n",
    "                labels = array(target_sequences), array(target_seq_lengths), array(num_questions), array(starts_list), array(stops_list)\n",
    "\n",
    "                self.data_que.put((features, labels))\n",
    "\n",
    "            else:\n",
    "                print('Queue full...Sleeping Again... 5 seconds')\n",
    "                sleep(self.global_sleep_clock)\n",
    "\n",
    "    def from_queue_generator(self):\n",
    "        while True:\n",
    "            if self.data_que.empty():\n",
    "                sleep(self.global_sleep_clock)\n",
    "            else:\n",
    "                features, labels = self.data_que.get()\n",
    "                yield features, labels\n",
    "\n",
    "        self.data_que.close()\n",
    "        self.data_que.join_thread()\n",
    "\n",
    "        self.terminate_processes()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "minimode=True\n",
    "\n",
    "datafile = 'data_containerV2_30msl.generator.pkl'\n",
    "with open(datafile, 'rb') as pick:\n",
    "    data = pickle.load(pick)\n",
    "\n",
    "# params & configs\n",
    "graph_params = load_graph_params(batch_size=batch_size, **data.return_config())\n",
    "\n",
    "input_fn_params = load_input_fn_params(\n",
    "    batch_size=batch_size if not minimode else int(1e6),\n",
    "    repeat=-1\n",
    ")\n",
    "\n",
    "\" Main input function -- use this one \"\n",
    "kwargs = {\n",
    "    'questions': data.questions,\n",
    "    'non_questions': data.non_questions,\n",
    "    'Q_size': 400,\n",
    "    'num_proc': 2,\n",
    "    'word2index': graph_params['word2index'],\n",
    "    'max_seq_length': graph_params['input_max_length'],\n",
    "    'max_num_questions': input_fn_params['max_num_questions'],\n",
    "    'max_num_elements': input_fn_params['max_num_elements'],\n",
    "    'randomize_num_questions': input_fn_params['randomize_num_questions']\n",
    "}\n",
    "gen_obj = GeneratorInputV3(**kwargs)  # This should take max a few seconds to prefill the queue\n",
    "\n",
    "# samples = list()\n",
    "# for i in range(10):\n",
    "#     samples.append(\n",
    "#         gen_obj._generate_sequence_(data.questions, \n",
    "#                                     data.non_questions, \n",
    "#                                     graph_params.get('input_max_length'), \n",
    "#                                     input_fn_params.get('max_num_questions'), \n",
    "#                                     input_fn_params.get('max_num_elements'))\n",
    "#     )\n",
    "#     break\n",
    "# for x in samples:\n",
    "#     print(x)\n",
    "#     print\n",
    "# generator = gen_obj.from_queue_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [(35, 'uninteresting moral skepticism hehe'), (78, 'criglcragl qualia are not empirical phenomena empirical refers to sensory data'), (31, 'what else does the op need<QQQ>')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_breaks(input_list):\n",
    "    starts = list()\n",
    "    stops = list()\n",
    "    \n",
    "    start = 0\n",
    "    for length, sentence in test:\n",
    "        stop = start + length\n",
    "        \n",
    "        if sentence.endswith('<QQQ>'):\n",
    "            starts.append(start)\n",
    "            stops.append(stop)\n",
    "            \n",
    "        start += length\n",
    "    \n",
    "    return starts, stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_breaks(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile = 'data_containerV2_30msl.generator.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(datafile, 'rb') as pick:\n",
    "    data = pickle.load(pick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = 'temp_fake_temp'\n",
    "args = None\n",
    "minimode = True\n",
    "\n",
    "# output dir\n",
    "_name = '_'.join([model_dir, 'minitest1']) if True else args.model_dir\n",
    "model_dir = make_model_dir(name=_name, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params & configs\n",
    "batch_size=10\n",
    "graph_params = load_graph_params(batch_size=batch_size, **data.return_config())\n",
    "input_fn_params = load_input_fn_params(\n",
    "    batch_size=10,\n",
    "    repeat=-1\n",
    ")\n",
    "estimator_config = load_estimator_config(save_every=100, log_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_fn_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.transformer.run_model import model_fn\n",
    "from backend.preprocess import _generate_sequence_, GeneratorInputV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = GeneratorInputV3(400, 6, data.questions, data.non_questions, data.word2index, 30, 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gener = gen.from_queue_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(gener)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load estimator\n",
    "classifier = tf.estimator.Estimator(model_fn=model_fn, params=graph_params, config=estimator_config, model_dir='fake2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_input_fn(graph_params, input_fn_params):\n",
    "    \" Main input function -- use this one \"\n",
    "    generator = gen.from_queue_generator()\n",
    "\n",
    "    msl = graph_params['input_max_length']\n",
    "    bsize = 2\n",
    "\n",
    "    dataset = tf.data.Dataset.from_generator(lambda: generator,\n",
    "                                             output_types=((tf.int32, tf.int32, tf.int32, tf.int32),\n",
    "                                                           (tf.int32, tf.int32, tf.int32, tf.int32, tf.int32)),\n",
    "                                             output_shapes=(\n",
    "                                                 (tf.TensorShape([msl]), tf.TensorShape([]), tf.TensorShape([msl]), tf.TensorShape([])),\n",
    "                                                 (tf.TensorShape([msl]), tf.TensorShape([]), tf.TensorShape([]), tf.TensorShape([]), tf.TensorShape([]))\n",
    "                                                 )\n",
    "    )\n",
    "    dataset = dataset.batch(bsize).prefetch(int(bsize * 3))\n",
    "    feature, label = dataset.make_one_shot_iterator().get_next()\n",
    "\n",
    "    features = {\n",
    "        'encoder_inputs': feature[0],  # encoder_inputs\n",
    "        'encoder_input_lengths': feature[1],  # encoder_input_lengths\n",
    "        'decoder_inputs': feature[2],  # decoder_inputs\n",
    "        'decoder_input_lengths': feature[3]  # decoder_input_lengths\n",
    "    }\n",
    "    labels = {\n",
    "        'target_sequences': label[0],  # target_sequences\n",
    "        'target_seq_lengths': label[1],  # target_seq_lengths\n",
    "        'num_questions': label[2],  # num_questions\n",
    "        'starts': label[3], # question start position\n",
    "        'stops': label[4]  # question stop position\n",
    "    }\n",
    "    return features, labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "classifier.train(input_fn=lambda: train_input_fn(graph_params, input_fn_params), steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.initializers.truncated_normal(0.0, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "three_channel = tf.constant(np.random.normal(0, 1, size=(2, 20, 50, 1)), tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = tf.layers.conv2d(three_channel, 126, (5, 5), activation=tf.nn.relu,use_bias=True, kernel_initializer=init, name='conv1')\n",
    "conv1_flat = tf.layers.flatten(conv1, name='flatten')\n",
    "\n",
    "key = tf.layers.dense(conv1_flat, units=512, activation=tf.nn.relu)\n",
    "query = tf.layers.dense(conv1_flat, units=512, activation=tf.nn.relu)\n",
    "value = tf.layers.dense(conv1_flat, units=512, activation=tf.nn.relu)\n",
    "\n",
    "scale = tf.sqrt(tf.constant(512.0, tf.float32))\n",
    "raw_scores = tf.divide(tf.cast(tf.matmul(key, query, transpose_b=True), tf.float32), scale)\n",
    "\n",
    "attention_scores = tf.nn.softmax(raw_scores, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "attended = tf.matmul(attention_scores, value)\n",
    "\n",
    "start_predictions = tf.layers.dense(attended, units=1, activation=tf.nn.sigmoid)\n",
    "start_predictions_transformed = transform_to_range(start_predictions, min_value=0, max_value=params['input_max_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
